{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from deslib.dcs import OLA\n",
    "# from deslib.dcs import KNORA_U, KNORA_E\n",
    "from deslib.des import  KNOP, METADES\n",
    "from deslib.static import SingleBest, StackedClassifier\n",
    "from deslib.static import StaticSelection\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_SET = {\n",
    "    \"feature\": 1,\n",
    "    \"permission\": 2,\n",
    "    \"activity\": 3,\n",
    "    \"service_receiver\": 3,\n",
    "    \"provider\": 3,\n",
    "    \"service\": 3,\n",
    "    \"intent\": 4,\n",
    "    \"api_call\": 5,\n",
    "    \"real_permission\": 6,\n",
    "    \"call\": 7,\n",
    "    \"url\": 8\n",
    "}\n",
    "\n",
    "\n",
    "def count_feature_set(lines):\n",
    "    \"\"\"\n",
    "    Count how many features belong to a specific set\n",
    "    :param lines: features in the text file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    features_map = {x: 0 for x in range(1, 9)}\n",
    "    for l in lines:\n",
    "        if l != \"\\n\":\n",
    "            set = l.split(\"::\")[0]\n",
    "            features_map[FEATURES_SET[set]] += 1\n",
    "    features = []\n",
    "    for i in range(1, 9):\n",
    "        features.append(features_map[i])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(LOAD_DATA=False):\n",
    "    if LOAD_DATA:\n",
    "        print(\"Previous data not loaded. Attempt to read data ...\")\n",
    "        mypath = r\"C:\\Users\\josev\\Desktop\\ProjetoPessoalNN\\GeorgeDarminton\\Drebin\\MetaData\\feature_vectors\\feature_vectors\"\n",
    "        onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "        print(\"Reading csv file for ground truth ...\")\n",
    "        ground_truth = np.loadtxt(r\"C:\\Users\\josev\\Desktop\\ProjetoPessoalNN\\GeorgeDarminton\\Drebin\\MetaData\\sha256_family.csv\", delimiter=\",\", skiprows=1, dtype=str)\n",
    "        # print ground_truth.shape\n",
    "        # families = np.unique(ground_truth[:, 1])\n",
    "        # print families\n",
    "        # print len(families)\n",
    "\n",
    "        print(\"Reading positive and negative texts ...\")\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for virus in tqdm(onlyfiles):\n",
    "            if virus in ground_truth[:, 0]:\n",
    "                pos.append(virus)\n",
    "            else:\n",
    "                #if len(neg) < 5560:\n",
    "                #if len(neg) < 22240:\n",
    "                    neg.append(virus)\n",
    "\n",
    "        print(\"Extracting features ...\")\n",
    "        x = []\n",
    "        y = []\n",
    "        for text_file in tqdm(pos):\n",
    "            sys.stdin = open(\"%s/%s\" % (mypath, text_file))\n",
    "            features = sys.stdin.readlines()\n",
    "            sample = count_feature_set(features)\n",
    "            x.append(sample)\n",
    "            y.append(1)\n",
    "\n",
    "        for text_file in tqdm(neg):\n",
    "            sys.stdin = open(\"%s/%s\" % (mypath, text_file))\n",
    "            features = sys.stdin.readlines()\n",
    "            sample = count_feature_set(features)\n",
    "            x.append(sample)\n",
    "            y.append(0)\n",
    "\n",
    "        print(\"Data is read successfully:\")\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        print(x.shape, y.shape)\n",
    "\n",
    "        print(\"Saving data under data_numpy directory ...\")\n",
    "        np.save(r\"C:\\Users\\josev\\Desktop\\ProjetoPessoalNN\\GeorgeDarminton/x_all.npy\", x)\n",
    "        np.save(r\"C:\\Users\\josev\\Desktop\\ProjetoPessoalNN\\GeorgeDarminton/y_all.npy\", y)\n",
    "\n",
    "        return x, y\n",
    "    else:\n",
    "        print(\"Loading previous data ...\")\n",
    "        x_ = np.load(r\"C:\\Users\\josev\\Desktop\\ProjetoPessoalNN\\GeorgeDarminton/x_all.npy\")\n",
    "        y_ = np.load(r\"C:\\Users\\josev\\Desktop\\ProjetoPessoalNN\\GeorgeDarminton/y_all.npy\")\n",
    "        print(x_.shape, y_.shape)\n",
    "        # print x == x_, y == y_\n",
    "        return x_, y_\n",
    "\n",
    "\n",
    "def map_family_to_category(families):\n",
    "    out = {}\n",
    "    count = 1\n",
    "    for family in families:\n",
    "        out[family] = count\n",
    "        count += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #x, y = read(LOAD_DATA=True)\n",
    "    x, y = read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, y_all = read(LOAD_DATA=False)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, stratify=y_all, random_state=42)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_predictions(base_path):\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Percorre todas as pastas dentro do diretório base\n",
    "    for model_name in os.listdir(base_path):\n",
    "        model_path = os.path.join(base_path, model_name)\n",
    "        \n",
    "        if os.path.isdir(model_path):\n",
    "            # Para cada modelo (30 execuções)\n",
    "            model_predictions = []\n",
    "            for i in range(1, 31):\n",
    "                file_path = os.path.join(model_path, f'model_{i}.csv')\n",
    "                \n",
    "                if os.path.isfile(file_path):\n",
    "                    # Carrega as previsões do modelo\n",
    "                    predictions = pd.read_csv(file_path).values.flatten()\n",
    "                    model_predictions.append(predictions)\n",
    "            \n",
    "            # Adiciona as previsões de todas as execuções desse modelo\n",
    "            all_predictions.append(np.array(model_predictions).mean(axis=0))\n",
    "\n",
    "    return np.array(all_predictions)\n",
    "\n",
    "def calculate_instance_hardness(all_predictions, true_labels):\n",
    "    # Verifica se cada previsão está correta\n",
    "    correctness_matrix = (all_predictions == true_labels).astype(int)\n",
    "    # Calcula quantas vezes cada instância foi classificada incorretamente\n",
    "    instance_hardness = 1 - correctness_matrix.mean(axis=0)\n",
    "    return instance_hardness\n",
    "\n",
    "# Caminhos para os modelos balanceados e desbalanceados\n",
    "unbalanced_path = r'E:\\DrebinStudy\\Unbalanced'\n",
    "balanced_path = r'E:\\DrebinStudy\\Balanced'\n",
    "\n",
    "# Suponha que os rótulos verdadeiros estão em um CSV\n",
    "true_labels = pd.read_csv(r'E:\\DrebinStudy\\true_labels.csv').values.flatten()\n",
    "\n",
    "# Carrega as previsões\n",
    "unbalanced_predictions = load_predictions(unbalanced_path)\n",
    "balanced_predictions = load_predictions(balanced_path)\n",
    "\n",
    "# Calcula Instance Hardness\n",
    "unbalanced_hardness = calculate_instance_hardness(unbalanced_predictions, true_labels)\n",
    "balanced_hardness = calculate_instance_hardness(balanced_predictions, true_labels)\n",
    "\n",
    "# Cria DataFrames para salvar os resultados\n",
    "df_unbalanced_hardness = pd.DataFrame({\n",
    "    'Instance': np.arange(len(true_labels)),\n",
    "    'Instance Hardness': unbalanced_hardness,\n",
    "    'True Label': true_labels\n",
    "})\n",
    "\n",
    "df_balanced_hardness = pd.DataFrame({\n",
    "    'Instance': np.arange(len(true_labels)),\n",
    "    'Instance Hardness': balanced_hardness,\n",
    "    'True Label': true_labels\n",
    "})\n",
    "\n",
    "# Salva os resultados em CSV\n",
    "df_unbalanced_hardness.to_csv(r'E:\\DrebinStudy\\Unbalanced_Instance_Hardness.csv', index=False)\n",
    "df_balanced_hardness.to_csv(r'E:\\DrebinStudy\\Balanced_Instance_Hardness.csv', index=False)\n",
    "\n",
    "print(\"Instance Hardness calculada e salva com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
